%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template for homework of Introduction to Machine Learning.
%
%  Fill in your name, lecture number, lecture date and body
%  of homework as indicated below.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,letter,notitlepage]{article}
%Mise en page
\usepackage[left=2cm, right=2cm, lines=45, top=0.8in, bottom=0.7in]{geometry}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage[UTF8]{ctex}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{1.5pt}
\newcommand\Loadedframemethod{TikZ}
\usepackage[framemethod=\Loadedframemethod]{mdframed}

\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{thmtools}
\newtheorem{lemma}{Lemma}

\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}

\usepackage{graphicx} % more modern
\usepackage{subfigure}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Define math operator %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{\bf argmin}
\DeclareMathOperator*{\relint}{\bf relint\,}
\DeclareMathOperator*{\dom}{\bf dom\,}
\DeclareMathOperator*{\intp}{\bf int\,}
%%%%%%%%%%%%%%%%%%%%%%%


\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}
\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{exercise}{\textbf{Exercise}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Problem environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{problem}{\textbf{Problem}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Solution Environment %%
%%%%%%%%%%%%%%%%%%%%%%%
\declaretheoremstyle
[
spaceabove=0pt, 
spacebelow=0pt, 
headfont=\normalfont\bfseries,
notefont=\mdseries, 
notebraces={(}{)}, 
headpunct={:\quad}, 
headindent={},
postheadspace={ }, 
postheadspace=4pt, 
bodyfont=\normalfont, 
qed=$\blacksquare$,
preheadhook={\begin{mdframed}[style=myframedstyle]},
	postfoothook=\end{mdframed},
]{mystyle}

\declaretheorem[style=mystyle,title=Solution,numbered=no]{solution}
\mdfdefinestyle{myframedstyle}{%
	topline=false,
	rightline=false,
	leftline=false,
	bottomline=false,
	skipabove=-6ex,
	leftmargin=-10,
	rightmargin=-10}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Solution environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%% Homework info.
\newcommand{\posted}{\text{Nov. 5, 2019}}       			%%% FILL IN POST DATE HERE
\newcommand{\due}{\text{Nov. 18, 2019}} 			%%% FILL IN Due DATE HERE
\newcommand{\hwno}{\text{4}} 		           			%%% FILL IN LECTURE NUMBER HERE


%%%%%%%%%%%%%%%%%%%%
%% Put your information here %%
%%%%%%%%%%%%%%%%%%%
\newcommand{\name}{\text{Bowen Zhang}}  	          			%%% FILL IN YOUR NAME HERE
\newcommand{\id}{\text{PB17000215}}		       			%%% FILL IN YOUR ID HERE
%%%%%%%%%%%%%%%%%%%%
%% End of the student's info %%
%%%%%%%%%%%%%%%%%%%


\newcommand{\proj}[2]{\textbf{P}_{#2} (#1)}
\newcommand{\lspan}[1]{\textbf{span}  (#1)  }
\newcommand{\rank}[1]{ \textbf{rank}  (#1)  }
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


\lhead{
	\textbf{\name}
}
\rhead{
	\textbf{\id}
}
\chead{\textbf{
		Homework \hwno
}}


\begin{document}
\vspace*{-4\baselineskip}
\thispagestyle{empty}


\begin{center}
{\bf\large Introduction to Machine Learning}\\
{Fall 2019}\\
University of Science and Technology of China
\end{center}

\noindent
Lecturer: Jie Wang  			 %%% FILL IN LECTURER HERE
\hfill
Homework \hwno             			
\\
Posted: \posted
\hfill
Due: \due
\\
Name: \name             			
\hfill
ID: \id						
\hfill

\noindent
\rule{\textwidth}{2pt}

\medskip





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY OF HOMEWORK GOES HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Notice, }to get the full credits, please show your solutions step by step.

\begin{exercise}[Support Vector Machine (SVM) for Linearly Separable Cases \textnormal{40pts}]
Given the training sample $\mathcal{D}=\{ (\textbf{x}_i,y_i) \}_{i=1}^n$, where $\textbf{x}_i \in \mathbb{R}^d$ and $y_i \in \{ -1,1 \}$. Let 
\begin{align*}
    \mathcal{D}^+=\{(\textbf{x}_i,y_i)\in\mathcal{D}:y_i=1\},\hspace{5mm}\mathcal{D}^-=\{(\textbf{x}_i,y_i)\in\mathcal{D}:y_i=-1\}.
\end{align*}
Assume that $\mathcal{D}^+$ and $\mathcal{D}^-$ are nonempty and the training sample $\mathcal{D}$ is linearly separable. We have shown in class that SVM can be written as
\begin{align}\label{prob:SVM-1}
	&\min_{\textbf{w},b}\,\,\frac{1}{2}\| \textbf{w} \|^2, \\
	&\text{ s.t. } \min_i y_{i} ( \langle \textbf{w}, \textbf{x}_i \rangle + b ) = 1. \nonumber
\end{align}
Moreover, we further transform the problem in (\ref{prob:SVM-1}) to
\begin{align}\label{prob:SVM}
    	&\min_{\textbf{w},b}\,\,\frac{1}{2}\| \textbf{w} \|^2, \\
    	&\text{ s.t. }\,\, y_{i} ( \langle \textbf{w}, \textbf{x}_i \rangle + b ) \geq 1, i=1,\ldots,n. \nonumber
\end{align}
We denote the feasible set of the problem in (\ref{prob:SVM}) by $$\mathcal{F}=\{(\mathbf{w},b):y_{i} ( \langle \textbf{w}, \textbf{x}_i \rangle + b ) \geq 1, i=1,\ldots,n\}.$$

\begin{enumerate}
    \item Show that $\mathcal{F}$ is nonempty.
    \item Show that the problem in (\ref{prob:SVM}) admits an optimal solution.
    \item Let $(\textbf{w}^*,b^*)$ be the optimal solution to problem (\ref{prob:SVM}). Show that $\mathbf{w}^*\neq0$.
    \item Show that the problems in (\ref{prob:SVM-1}) and (\ref{prob:SVM}) are equivalent, that is, they share the same set of optimal solutions.
    \item Let $(\textbf{w}^*,b^*)$ be the optimal solution to problem (\ref{prob:SVM}). Show there exist at least one positive sample and one negative sample, respectively, such that the equality holds. In other words, there exist $i,j \in \{ 1,2,\dots,n \}$ such that 
    \begin{align*}
        1=y_i = &\langle \textbf{w}^*, \textbf{x}_i \rangle + b^*, \\
        -1=y_j = &\langle \textbf{w}^*, \textbf{x}_j \rangle + b^*.
    \end{align*}
    \item Show that the optimal solution to problem (\ref{prob:SVM}) is unique.
    \item Find the dual problem of (\ref{prob:SVM}) and the corresponding optimal conditions.
\end{enumerate}

\end{exercise}
\begin{solution}
	\heiti
	\begin{enumerate}
		\ \\
		\item
			由样本线性可分可知存在 $w_{0} \in \mathbb{R}^d$ 使得：\\
			$\begin{aligned} {\langle w_{0}, x_{i}\rangle > 0 \quad x_{i} \in D^{+}}  \\ {\langle w_{0}, x_{i}\rangle < 0   \quad x_{i} \in D^{-}} \end{aligned}$\\
			所以存在 $\varepsilon, k > 0$ 使得：\\
			\begin{align*}
				y_{i} \langle w_{0}, x_{i}\rangle &\geqslant \varepsilon \\ y_{i}\langle w_{0}, x_{i}\rangle &\geqslant \varepsilon \\  \Rightarrow \quad y_{i}\langle \frac{w_{0}}{\varepsilon}, x_{i}\rangle &\geqslant 1
			\end{align*}
			所以令 $w = \frac{w_{0}}{\varepsilon}, b = 0$ 可知 $\mathcal{F}$ 非空。
		\item 
			目标函数只能在有限区间内取到最小值（$\mathbf{w} \rightarrow \infty$ 时，目标函数也趋于无穷），而解集非空，故一定存在满足约束条件的最优解。
		\item
			假如 $\mathbf{w}^* = 0$ ，那么有：
			\begin{align*}
				y_i b \geqslant 1, i = 1,\ldots,n.
			\end{align*}
			对 $ y_i = 1$ 有 $b \geqslant 1$，对 $y_i = -1$ 有 $b \leqslant -1$矛盾，故 $\mathbf{w}^* \neq 0$.
		\item 如果两个问题等价，那么当目标函数取到最小值时，$\exists i$ 使得 $y_{i} ( \langle \textbf{w}, \textbf{x}_i \rangle + b ) = 1$.假设问题不等价，那么设问题 2 的最优解为 $(w^*, b^*)$，满足：
		\begin{align*}
			y_{i} ( \langle \textbf{w}^{*}, \textbf{x}_i \rangle + b^{*} ) \geqslant k > 1, i = 1,\ldots,n
		\end{align*}
		那么有：
		\begin{align*}
			y_{i} ( \langle \frac{\textbf{w}^{*}}{k}, \textbf{x}_i \rangle + \frac{b^*}{k} ) \geqslant 1, i = 1,\ldots,n
		\end{align*}
		此时目标函数的值为 $\frac{1}{2}\|\frac{\mathbf{w}^{*}}{k}\|^{2}$，这与 $w^*$ 是最优解矛盾，即问题 2 取到最优解时两个问题的约束条件相同，故两个问题等价。
		\item 由 4 可知必 $\exists i, y_{i} ( \langle \textbf{w}^{*}, \textbf{x}_i \rangle + b^{*} ) = 1$，不妨设这个样本是负样本（正样本证明过程类似，所以只选负样本证明）。如果对所有的正样本都有：
		\begin{align*}
			\langle \textbf{w}^{*}, \textbf{x}_i \rangle + b^{*}  \geqslant k > 1, \textbf{x}_i \in \mathcal{D}^+
		\end{align*}
		对所有的负样本有：
		\begin{align*}
			\langle \textbf{w}^{*}, \textbf{x}_i \rangle + b^{*}  \leqslant -1, \textbf{x}_i \in \mathcal{D}^-
		\end{align*}
		取 $0 < t < k - 1$，稍作变换得：
		\begin{align*}
			\langle \textbf{w}^{*}, \textbf{x}_i \rangle + b^{*} - t  \geqslant k - t > 1, \textbf{x}_i \in \mathcal{D}^+\\
			\langle \textbf{w}^{*}, \textbf{x}_i \rangle + b^{*} - t  \leqslant -1 - t, \textbf{x}_i \in \mathcal{D}^-
		\end{align*}
		即：
		\begin{align*}
			b_0 &= b^{*} - t\\
			y_{i} ( \langle \textbf{w}^{*}, \textbf{x}_i \rangle + b_0 ) &> 1, i = 1,\ldots,n
		\end{align*}
		那么同 4 可证存在更小的 $w$ 使得目标函数取得最小值，这与 $w^*$ 使得目标函数最小矛盾，故命题成立。
		\item 假设存在 $(w_1^*,b_1^*), (w_2^*, b_2^*)$ 两组不同的最优解。假设 $w_1^* \neq w_2^*$，那么有：
		\begin{align*}
			y_{i} ( \langle \textbf{w}_1^{*}, \textbf{x}_i \rangle + b_1^* ) &\geqslant 1, i = 1,\ldots,n\\
			y_{i} ( \langle \textbf{w}_2^{*}, \textbf{x}_i \rangle + b_2^* ) &\geqslant 1, i = 1,\ldots,n\\
			\Rightarrow y_{i} ( \langle \textbf{w}_1^{*} + \textbf{w}_2^{*}, \textbf{x}_i \rangle + b_1^* + b_2^* ) &\geqslant 2, i = 1,\ldots,n\\
			\Rightarrow y_{i} ( \langle \frac{\textbf{w}_1^{*} + \textbf{w}_2^{*}}{2}, \textbf{x}_i \rangle + \frac{b_1^* + b_2^*}{2} ) &\geqslant 1, i = 1,\ldots,n
		\end{align*}
		而
		\begin{align*}
			\| \frac{\textbf{w}_1^{*} + \textbf{w}_2^{*}}{2} \|^2 \leqslant \frac{\| \textbf{w}_1^*\|^2 + \|\textbf{w}_2^* \|^2}{2} =  \|\textbf{w}_1^* \|^2 = \|\textbf{w}_2^* \|^2
		\end{align*}
		因为假设 $w_1^* \neq w_2^*$，所以上式为严格小于。取 $w = \frac{{w}_1^{*} + {w}_2^{*}}{2}$，这说明 $w_1^*, w_2^*$ 不是让目标函数最小的 $w$，所以与 $(w_1^*,b_1^*), (w_2^*, b_2^*)$ 是两组最优解矛盾，故必有 $w_1^* = w_2^*$ 成立。\\
		若 $b_1^* \neq b_2^*$，不妨设 $b_1^* > b_2^*$，由 5 可知存在 $i \in \{1, 2,\dots, n\}$：
		\begin{align*}
			1=y_i &= \langle \textbf{w}^*, \textbf{x}_i \rangle + b_1^*, y_i \in \mathcal{D}^+\\
			&> \langle \textbf{w}^*, \textbf{x}_i \rangle + b_2^*
		\end{align*}
		这与约束条件矛盾，故必有 $b_1^* = b_2^*$。综上最优解唯一。
		\item 引入拉格朗日乘子对偶问题为：
		\begin{align*}
			{ L ( w , b , \alpha ) = \frac { 1 } { 2 } \| w \| ^ { 2 } + \sum _ { i = 1 } ^ { N } \alpha _ { i } \left[ - y _ { i } \left( \langle w \cdot x _ { i } \rangle + b \right) + 1 \right] } \\ { = \frac { 1 } { 2 } \| w \| ^ { 2 } - \sum _ { i = 1 } ^ { N } \alpha _ { i } y _ { i } \left(\langle w, x_i\rangle + b \right) + \sum _ { i = 1 } ^ { N } \alpha _ { i } }
		\end{align*}
		要求解的最优问题为：
		\begin{align*}
			\max_{\alpha} \min_{w, b} L ( w , b , \alpha )
		\end{align*}
		令 $L(w, b, \alpha)$ 对 $w, b$ 的偏导为 0 得：
		\begin{align*}
			w = \sum\limits_{i=1}^{N}\alpha_iy_ix_i,\\
			0 = \sum\limits_{i=1}^{N}\alpha_iy_i.
		\end{align*} 
		代入得到对偶问题为：
		\begin{align*}
			\max_{\alpha} \sum\limits_{i=1}^{N} \alpha_i - \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\alpha_i\alpha_jy_iy_j\langle x_i, x_j \rangle\\
			s.t. \sum\limits_{i=1}^{N}\alpha_iy_i = 0,\\
			\alpha_i \geqslant 0, i= 1,\ldots,n
		\end{align*}
	\end{enumerate}
\end{solution}

\newpage
\begin{exercise}[Visualization Lemma]
Consider the primal problem as follows.
\begin{align}\label{prob:primal-vec}
\min_{\mathbf{x}}\,&f(\mathbf{x})\\ \nonumber
{\rm s.t.}\,&\mathbf{g}(\mathbf{x})\leq0,\\ \nonumber
&\mathbf{h}(\mathbf{x})=0, \\ \nonumber
&\mathbf{x}\in X,
\end{align}
where $\mathbf{x}\in\mathbb{R}^n$, $f:\mathbb{R}^n\rightarrow\mathbb{R}$, $\mathbf{g}:\mathbb{R}^n\rightarrow\mathbb{R}^m$, $\mathbf{h}:\mathbb{R}^n\rightarrow\mathbb{R}^p$, and $X\subseteq\mathbb{R}^n$. The functions $f$, $\mathbf{g}$, and $\mathbf{h}$ are continuously differentiable. 

The Lagrangian $L:\mathbb{R}^n\times \mathbb{R}^m\times \mathbb{R}^p\rightarrow\mathbb{R}$ associated with the problem in (\ref{prob:primal-vec}) takes the form of
\begin{align}\label{def:Lagrangian}
	L(\mathbf{x},\lambda,\mu)=f(\mathbf{x})+\sum_{i=1}^m\lambda_ig_i(\mathbf{x})+\sum_{i=1}^p\mu_ih_i(\mathbf{x}).
\end{align}

Let
\begin{align}\label{def:image-set}
	\mathbb{R}^{m+p+1}\supseteq S=\{(\mathbf{g}(\mathbf{x}),\mathbf{h}(\mathbf{x}),f(\mathbf{x})):\mathbf{x}\in X\}.
\end{align}

Show that the results as follows (hint: see Fig. \ref{fig:visualization-lemma}).

\begin{lemma}
	\textbf{\textup {Visualization Lemma}}
	\begin{enumerate}
		\item The hyperplane with normal $(\lambda,\mu,1)$ that passes through a vector $(\mathbf{g}(\mathbf{x}),\mathbf{h}(\mathbf{x}),f(\mathbf{x}))$ intercepts the vertical axis $\{(\mathbf{0},z):z\in\mathbb{R}\}$ at the level $L(\mathbf{x},\lambda,\mu)$.
		
		\item Among all hyperplanes with normal $(\lambda,\mu,1)$ that contains in their positive halfspace the set $S$ defined in (\ref{def:image-set}), the highest attained level of interception of the vertical axis is $\inf_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda,\mu)$.
		
		\item $(\lambda^*,\mu^*)$ is a geometric multiplier if and only if $\lambda^*\geq0$ and among all hyperplanes with normal $(\lambda^*,\mu^*,1)$ that contain in their positive halfspace the set $S$, the highest attained level of of interception of the vertical axis is $f^*$, where
		\begin{align*}
		    f^*=\min\{f(\mathbf{x}):\mathbf{g}(\mathbf{x})\leq0,\mathbf{h}(\mathbf{x})=0,\mathbf{x}\in X)\}.
		\end{align*}
	\end{enumerate}
\end{lemma}
\end{exercise}
\begin{figure}[!h] 
	\centering{
		\subfigure[] { \label{fig:geometric-multiplier-0}
			\includegraphics[width=0.45\columnwidth]{figures/geometric-multipliers-0.pdf}
		}
		\subfigure[] { \label{fig:geometric-multiplier-1}
			\includegraphics[width=0.45\columnwidth]{figures/geometric-multipliers-1.pdf}
		}
	}
	\caption{Illustration of the visualization lemma with one inequality constraint.}
	\label{fig:visualization-lemma}
\end{figure}



\newpage
${}$
\begin{solution}
	\heiti
	\begin{enumerate}
		\ \\
		\item 设空间的坐标为 $(\textbf{u}, \textbf{v}, w)$ ，其中 $\textbf{u} \in \mathbb{R}^m, \textbf{v} \in \mathbb{R}^p, w \in \mathbb{R}$，超平面的方程可以写为
		\begin{align*}
			F = \lambda \textbf{u} + \mu \textbf{v} + w
		\end{align*}
		因为超平面过 $(\mathbf{g}(\mathbf{x}),\mathbf{h}(\mathbf{x}),f(\mathbf{x}))$ ，则有：
		\begin{align*}
			\lambda \textbf{u} + \mu \textbf{v} + w = \lambda \mathbf{g}(\mathbf{x}) + \mu \mathbf{h}(\mathbf{x}) + f(\mathbf{x})
		\end{align*}
		可以看出当 $\textbf{u} = \textbf{v} = \textbf{0}$ 时，超平面交坐标轴于 $\lambda \mathbf{g}(\mathbf{x}) + \mu \mathbf{h}(\mathbf{x}) + f(\mathbf{x})$，此即为 $L(\mathbf{x},\lambda,\mu)$。
		\item 根据题目要求，超平面方程必须满足：
		\begin{align*}
			\lambda \textbf{u} + \mu \textbf{v} + w \leqslant \lambda \mathbf{g}(\mathbf{x}) + \mu \mathbf{h}(\mathbf{x}) + f(\mathbf{x})
		\end{align*}
		所以与坐标轴的交点满足：
		\begin{align*}
			w \leqslant \lambda \mathbf{g}(\mathbf{x}) + \mu \mathbf{h}(\mathbf{x}) + f(\mathbf{x})
		\end{align*}
		所能取到的最大值满足：
		\begin{align*}
			w = \inf_{\mathbf{x}\in X}\, \lambda \mathbf{g}(\mathbf{x}) + \mu \mathbf{h}(\mathbf{x}) + f(\mathbf{x}) = \inf_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda,\mu)
		\end{align*}
		当超平面与 $S$ 有切点时取到等号。
		\item $\lambda^*, \mu^*$ 使得 $\inf_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda^*,\mu^*) = f^*$，由于 $\mathbf{x} \in X$，所以有 $\mathbf{g}(\mathbf{x}) \leqslant 0, \mathbf{h}(\mathbf{x}) = 0$，则有
		\begin{align*}
			&\inf_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda^*,\mu^*)\\
			= &\inf_{\mathbf{x}\in X}\,\{f(\mathbf{x})+\lambda^* \mathbf{g}(\mathbf{x})+\mu^* \mathbf{h}(\mathbf{x})\} = f^*\\
			= &\inf_{\mathbf{x}\in X}\,\{f(\mathbf{x})+\lambda^* \mathbf{g}(\mathbf{x})\}
		\end{align*}
		因为 $\lambda^* \geqslant 0$，所以 $\lambda^* \mathbf{g}(\mathbf{x}) \leqslant 0$，可以取 $\lambda^* \mathbf{g}(\mathbf{x}) = 0$，则
		\begin{align*}
			&\inf_{\mathbf{x}\in X}\,\{f(\mathbf{x})+\lambda^* \mathbf{g}(\mathbf{x})\} \\
			= &\inf_{\mathbf{x}\in X}\,\{f(\mathbf{x})\}\\
			= &\min\{f(\mathbf{x}):\mathbf{g}(\mathbf{x})\leq0,\mathbf{h}(\mathbf{x})=0,\mathbf{x}\in X)\}
		\end{align*}
	\end{enumerate}
\end{solution}

\newpage
\begin{exercise}[Geometric Multiplier]
Let $(\lambda^*,\mu^*)$ be a geometric multiplier. Show that $\mathbf{x}^*$ is a global minimum of the primal problem (\ref{prob:primal-vec}) if and only if $\mathbf{x}^*$ is feasible and 
\begin{align*}
	&\mathbf{x}^*\in\argmin_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda^*,\mu^*),\\
	&\lambda_i^*g_i(\mathbf{x}^*)=0,\,i=1,\ldots,m.
\end{align*}
\end{exercise}

\begin{solution}
	\heiti
	\ \\
	$\mathbf{x}^* \in X$ 说明 $\mathbf{g}(\mathbf{x^*}) \leqslant 0, \mathbf{h}(\mathbf{x^*}) = 0$，$\mathbf{x}^*$ 是问题 3 的最小值，则 $f^* = f(\mathbf{x}^*) = \min\{f(\mathbf{x}):\mathbf{g}(\mathbf{x})\leq0,\mathbf{h}(\mathbf{x})=0,\mathbf{x}\in X)\}$，根据 $(\lambda^*,\mu^*)$ 是 geometric multiplier 可得：
	\begin{align*}
		\min\{f(\mathbf{x}):\mathbf{g}(\mathbf{x})\leq0,\mathbf{h}(\mathbf{x})=0,\mathbf{x}\in X)\} = f^* = & \inf_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda^*,\mu^*)\\
		= & \inf_{\mathbf{x}\in X}\,\{f(\mathbf{x})+\lambda^* \mathbf{g}(\mathbf{x})\}
	\end{align*}
	因为 $\lambda^* \geqslant 0$，所以 $\lambda^* \mathbf{g}(\mathbf{x}) \leqslant 0$，要使上式成立，那么 $\lambda^* \mathbf{g}(\mathbf{x}) = 0$，得：
	\begin{align*}
		L(\mathbf{x}^*,\lambda^*,\mu^*) = \min_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda^*,\mu^*)
	\end{align*}
	所以 $\mathbf{x}^*\in\argmin_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda^*,\mu^*)$ 并且 $\lambda^* \mathbf{g}(\mathbf{x}) = 0$。
\end{solution}

\newpage
\begin{exercise}[Lagrange Dual Problem]
Consider the primal problem (\ref{prob:primal-vec}) and the Lagrangian (\ref{def:Lagrangian}). We define the dual function for $(\lambda,\mu)\in\mathbb{R}^{m+p}$ by
\begin{align*}
	q(\lambda,\mu)=\inf_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda,\mu).
\end{align*}
The domain of $q$ is 
\begin{align*}
    \dom(q)=\{(\lambda,\mu):q(\lambda,\mu)>-\infty\}.
\end{align*}
The dual problem is
\begin{align*}
	\sup\,&q(\lambda,\mu),\\
	\mbox{s.t. }&\lambda\geq0.
\end{align*}
\begin{enumerate}
    \item Show that $\dom(q)$ is convex.
    \item Show that $-q(\lambda,\mu)$ is a convex function.
\end{enumerate}
\end{exercise}

\begin{solution}
	\heiti
	\ \\
	\begin{enumerate}
		\item 任取 $(\lambda_1, \mu_1), (\lambda_2, \mu_2) \in \dom(q)$，对于 $\forall \theta \in (0,1)$，考虑 $\theta(\lambda_1, \mu_1) + (1-\theta)(\lambda_2, \mu_2) = (\theta \lambda_1 + (1-\theta)\lambda_2, \theta \mu_1 + (1-\theta)\mu_2)$，有：
		\begin{align*}
			q(\theta \lambda_1 + (1-\theta)\lambda_2, \theta \mu_1 + (1-\theta)\mu_2) &=\inf_{\mathbf{x}\in X}\,L(\mathbf{x},\theta \lambda_1 + (1-\theta)\lambda_2, \theta \mu_1 + (1-\theta)\mu_2)\\
			&=\inf_{\mathbf{x}\in X}\,f(\mathbf{x})+(\theta \lambda_1 + (1-\theta)\lambda_2)g(\mathbf{x}) + (\theta \mu_1 + (1-\theta)\mu_2)h(\mathbf{x})\\
			&=\inf_{\mathbf{x}\in X}\,\theta L(\mathbf{x},\lambda_1, \mu_1) + (1-\theta)L(\mathbf{x}, \lambda_2, \mu_2)
		\end{align*}
		由于 $\inf_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda_1,\mu_1) > -\infty, \inf_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda_2,\mu_2) > -\infty$，故 $L(\mathbf{x},\lambda_1,\mu_1) > -\infty, L(\mathbf{x},\lambda_2,\mu_2) > -\infty$，所以 $\inf_{\mathbf{x}\in X}\,\theta L(\mathbf{x},\lambda_1, \mu_1) + (1-\theta)L(\mathbf{x}, \lambda_2, \mu_2) > -\infty$。由此可知 $\theta(\lambda_1, \mu_1) + (1-\theta)(\lambda_2, \mu_2) \in \dom(q)$，故是凸集。
		\item 任取 $(\lambda_1, \mu_1), (\lambda_2, \mu_2) \in \dom(q)$，对于 $\forall \theta \in (0,1)$，需要证明：
		\begin{align*}
			-q(\theta \lambda_1 + (1-\theta)\lambda_2, \theta \mu_1 + (1-\theta)\mu_2) &\leqslant -\theta q(\lambda_1,\mu_1) - (1-\theta)q(\lambda_2,\mu_2)\\
			\Leftrightarrow \inf_{\mathbf{x}\in X}\,\theta L(\mathbf{x},\lambda_1, \mu_1) + (1-\theta)L(\mathbf{x}, \lambda_2, \mu_2) &\geqslant \theta \inf_{\mathbf{x}\in X}\,  L(\mathbf{x},\lambda_1, \mu_1) + (1-\theta) \inf_{\mathbf{x}\in X}\, L(\mathbf{x}, \lambda_2, \mu_2)\\
		\end{align*}
		利用 $\theta \inf_{\mathbf{x}\in X}\,  L(\mathbf{x},\lambda_1, \mu_1) = \inf_{\mathbf{x}\in X}\, \theta L(\mathbf{x},\lambda_1, \mu_1), (1-\theta) \inf_{\mathbf{x}\in X}\, L(\mathbf{x}, \lambda_2, \mu_2) =  \inf_{\mathbf{x}\in X}\, (1-\theta) L(\mathbf{x}, \lambda_2, \mu_2)$，以及对于任意函数 $P,Q$ 有 $\inf(P+Q) \geqslant \inf P\,+ \inf Q$ 得：
		\begin{align*}
			\inf_{\mathbf{x}\in X}\,\theta L(\mathbf{x},\lambda_1, \mu_1) + (1-\theta)L(\mathbf{x}, \lambda_2, \mu_2) \geqslant  \inf_{\mathbf{x}\in X}\, \theta L(\mathbf{x},\lambda_1, \mu_1) +  \inf_{\mathbf{x}\in X}\,(1-\theta) L(\mathbf{x}, \lambda_2, \mu_2)
		\end{align*}
		成立，所以原命题成立。
	\end{enumerate}
\end{solution}



\newpage
\begin{exercise}[Duality Gap]
Duality gap is defined by
	\begin{align*}
		f^*-q^*.
	\end{align*}
	Show that the following results hold.
	\begin{enumerate}
		\item If there is no duality gap, the set of geometric multipliers is equal to the set of optimal dual solutions.
		\item If there is duality gap, the set of geometric multipliers is empty.
	\end{enumerate}
\end{exercise}

\begin{solution}
	\heiti
	\ \\
	\begin{enumerate}
		\item 由题，$f^* = q^*$，考虑 $q^*$:
		\begin{align*}
			q^* = \sup q(\lambda^*,\mu^*),\\
			s.t. \lambda^* \geq 0
		\end{align*}
		对于
		\begin{align*}
			q(\lambda,\mu) &= \inf_{\mathbf{x}\in X}\, L(\mathbf{x},\lambda,\mu)\\
			&= \inf_{\mathbf{x}\in X}\, \{f(\mathbf{x}) + \lambda \mathbf{g}(\mathbf{x}) + \mu \mathbf{h}(\mathbf{x})\}\\
			&\leqslant f(\mathbf{x}) + \lambda \mathbf{g}(\mathbf{x}) + \mu \mathbf{h}(\mathbf{x})
		\end{align*}
		对于 $\mathbf{x} \in X$，有 $\mathbf{h}(\mathbf{x}) = 0, \mathbf{g}(\mathbf{x}) \leqslant 0$，那么有 $\lambda\mathbf{g}(\mathbf{x}) \leqslant 0$，所以
		\begin{align*}
			q(\lambda,\mu) &\leqslant  f(\mathbf{x})\\
			\Rightarrow q^* =  \sup q(\lambda,\mu) &\leqslant \min f(\mathbf{x}) = f^*
		\end{align*}
		根据定义，如果 $(\lambda^*,\mu^*)$ 是 geometric multiplier，那么
		\begin{align*}
			q(\lambda^*,\mu^*) = f^*
		\end{align*}
		根据上面的推导
		\begin{align*}
			q(\lambda^*,\mu^*) \leqslant \sup q(\lambda,\mu) &= q^* \leqslant \min f(\mathbf{x})  \leqslant f^*\\
			\Rightarrow q(\lambda^*,\mu^*) &= q^* = f^*
		\end{align*}
		故综上，所有最优解 $(\lambda^*,\mu^*)$ 都满足 geometric multiplier 的定义，满足 geometric multiplier 定义的 $(\lambda^*,\mu^*)$ 同样是最优解，故命题成立。
		\item 假设 geometric multipliers 的集合不是空集，根据其定义
		\begin{align*}
			q(\lambda^*,\mu^*) = f^*
		\end{align*}
		由 1 的推导可推出 $q^* = q(\lambda^*,\mu^*) = f^*$，但这与 $f^* - q^* \neq 0$ 矛盾，故集合为空。
	\end{enumerate}
\end{solution}

\newpage
\begin{exercise}[Optimality Conditions]
	Show that a pair $\mathbf{x}^*$ and $(\lambda^*,\mu^*)$ is an optimal solution and geometric multiplier pair if and only if
	\begin{align}\label{cond:primal-feasibility}
		\mathbf{x}^*\in X,\,\mathbf{g}(\mathbf{x}^*)\leq0,\mathbf{h}(\mathbf{x}^*)=0,\hspace{18.5mm}(\mbox{\textup{Primal Feasibility}}),&\\ \label{cond:dual-feasibility}
		\lambda^*\geq0,\hspace{22mm}(\mbox{\textup{Dual Feasibility}}),&\\ \label{cond:lagrangian-optimality}
		\mathbf{x}^*\in\argmin_{\mathbf{x}\in X}\,L(\mathbf{x},\lambda^*,\mu^*),\hspace{10mm}(\mbox{\textup{Lagrangian Optimality}}),&\\ \label{cond:complementary-slackness}
		\lambda^*_ig_i(\mathbf{x}^*)=0,\,i=1,\ldots,m,\hspace{5mm}(\mbox{\textup{Complementary Slackness}}).&
	\end{align}
\end{exercise}


\newpage
\begin{exercise}[Saddle Point Interpretation]
	 Show that a pair $\mathbf{x}^*$ and $(\lambda^*,\mu^*)$ is an optimal solution-geometric multiplier pair if and only if $\mathbf{x}^*\in X$, $\lambda^*\geq0$, and $(\mathbf{x}^*,\lambda^*,\mu^*)$ is a saddle point of the Lagrangian, in the sense that
	\begin{align}\label{cond:saddle-Lagrangian}
		L(\mathbf{x}^*,\lambda,\mu)\leq L(\mathbf{x}^*,\lambda^*,\mu^*)\leq L(\mathbf{x},\lambda^*,\mu^*),\,\forall\,\mathbf{x}\in X,\,\lambda\geq0.
	\end{align}
\end{exercise}


\newpage
\begin{exercise}
     Recall that the soft margin SVM takes the form of
\begin{align}\label{prob:soft-SVM}
\min_{\mathbf{w},b,\xi}\,&\frac{1}{2}\|\mathbf{w}\|^2+C\sum_{i=1}^n\xi_i,\\ \nonumber
{\rm s.t.\,}&\,y_i(\langle\mathbf{w},\mathbf{x}_i\rangle+b)\geq1-\xi_i,i=1,\ldots,n,&\\ \nonumber
&\,\xi_i\geq0,\,i=1,\ldots,n,
\end{align}
where $C>0$.
The corresponding dual problem is
\begin{align}\label{prob:dual-soft-SVM}
		\min_{\alpha}\,&\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\,\alpha_i\alpha_jy_iy_j\langle\mathbf{x}_i,\mathbf{x}_j\rangle-\sum_{i=1}^n\alpha_i\\\nonumber
	\mbox{s.t. }\,&\sum_{i=1}^n\alpha_iy_i=0,\\\nonumber
	&\alpha_i\in[0,C],i=1,\ldots,n.
\end{align}
\begin{enumerate}
    \item Show that the problems (\ref{prob:soft-SVM}) and (\ref{prob:dual-soft-SVM}) always admit optimal solutions.
    
    \item Suppose that the training data consists of two data instances $x_1=1$ and $x_2=-1$, and the corresponding labels are $y_1=1$ and $y_2=-1$.
    \begin{enumerate}
        \item Please solve problem (\ref{prob:SVM}) and find the primal and dual optimal solutions.
        \item Please solve problem (\ref{prob:soft-SVM}) with $C<\frac{1}{2}$ and find the primal and dual optimal solutions.
    \end{enumerate}

    \item Suppose that the training data consists of four data instances: $\mathbf{x}_1=(2,3)$, $\mathbf{x}_2=(1,2)$, $\mathbf{x}_3=(1,3)$, and $\mathbf{x}_4=(2,2)$, and the corresponding labels are $y_1=y_2=1$ and $y_3=y_4=-1$. Please solve the problem in (\ref{prob:dual-soft-SVM}) with $C=10$.
\end{enumerate}
Notice that, for the last two parts, you need to find all the primal and dual optimal solutions if they are not unique.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
